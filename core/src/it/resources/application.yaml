# Copyright (c) 2017 ~ 2025, the original author James Wong individual Inc,
# All rights reserved. Contact us <jameswong1376@gmail.com>
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# #### Environment Base configuration. ####
#

spring:
  application.name: stream-connect
  main:
    allow-circular-references: true

server:
  port: 8000
  servlet:
    contextPath: /

management:
  server:
    address: 0.0.0.0
    port: 10108
    ssl:
      enabled: false
    servlet.contextPath: /
  security:
    enabled: false
    sensitive: true
  metrics:
    export:
      prometheus:
        enabled: true
      jmx:
        enabled: true
  endpoints:
    web:
      basePath: /actuator
      exposure:
        include: '*'

stream-connect:
  definitions:
    bases:
      consumerProps: &baseConsumerProps
        request.timeout.ms: 30000
        auto.offset.reset: earliest
      executorProps: &baseExecutorProps
        sharedExecutorThreadPoolSize: 50
        sharedExecutorQueueSize: 500
        sequenceExecutorsMaxCountLimit: 10
        sequenceExecutorsPerQueueSize: 100
        executorWarmUp: true

    checkpoints:
      - !KAFKA
        name: checkpoint_1
        executorProps:
          <<: *baseExecutorProps
        topicPrefix: "test_streamconnect_topic_ckp_"
        topicPartitions: 10
        replicationFactor: 1
        # TODO support directly parse to Enum
        qosType: 3 # 0:AT_MOST_ONCE|1:RETRIES_AT_MOST|2:RETRIES_AT_MOST_STRICTLY|3:STRICTLY
        qoSMaxRetries: 5
        qoSMaxRetriesTimeout: 1800000 # 30min
        producerMaxCountLimit: 100
        producerProps:
          #bootstrap.servers: ${IT_KAFKA_SERVERS_01:localhost:9092} # By default: {.source.consumerProps.'bootstrap.servers'}
          acks: "0"
          request.timeout.ms: 30000
          max.request.size: 1048576
          send.buffer.bytes: 131072
          retries: 5
          retry.backoff.ms: 6000
          compression.type: gzip
        defaultTopicProps:
          cleanup.policy: "delete"
          retention.ms: 86400000 # By default: 1day
          retention.bytes: 1073741824 # By default: 1G

    sources:
      - !STATIC_SOURCE
        name: subscribeSource_1
        staticConfigs:
          - name: kafka01
            topicPattern: "test_streamconnect_input_shared"
            parallelism: 2
            consumerProps:
              bootstrap.servers: ${IT_KAFKA_SERVERS_01:localhost:9092}
              group.id: "test_streamconnect_source_01"
              <<: *baseConsumerProps
          - name: kafka02
            topicPattern: "test_streamconnect_input_t1003"
            parallelism: 2
            consumerProps:
              bootstrap.servers: ${IT_KAFKA_SERVERS_02:localhost:9092}
              group.id: "test_streamconnect_source_02"
              <<: *baseConsumerProps
      #- !JDBC
      #  name: subscribeSource_2
      #  jdbcConfig:
      #    jdbcUrl: jdbc:postgresql://localhost:5432/subscriber
      #    username: subscriber
      #    password: 123456
      #    sql: 'select * from t_subscriber'

    filters:
      - !STANDARD_FILTER
        name: subscribeFilter_1
        filterProps:
          updateMergeConditionsDelayTime: 2000

    mappers:
      - !STANDARD_MAPPER
        name: subscribeMapper_1

    sinks:
      - !NOOP_SINK
        name: sink_1
        sinkConfig:
          groupIdPrefix: "test_streamconnect_group_sink_"
          parallelism: 2
          executorProps:
            <<: *baseExecutorProps
          consumerProps:
            bootstrap.servers: ${IT_KAFKA_SERVERS_01:localhost:9092}
            <<: *baseConsumerProps

    ## Notice: The following configuration method is usually used in experimental scenarios to
    ## define the subscriber list. When integrating into actual business applications, the interface
    ## 'com.wl4g.streamconnect.facade.StreamConnectEngineFacade' should be customized to get subscribers info from the DB.
    subscribers:
      - id: s1001
        name: "subscriber_1"
        enable: true
        rule:
          isSequence: false
          logRetentionTime: 1d
          logRetentionSize: 10GB
          policies:
            - tenantId: t1001
              recordFilter: '{"type":"RELATION","name":"testCondition1","fn":{"expression":"cts > 1690345000000 && __properties__.__online__.connected == true"}}'
              fieldFilter:
          properties: { }
      - id: s1002
        name: "subscriber_2"
        enable: true
        rule:
          isSequence: false
          logRetentionTime: 1d
          logRetentionSize: 10GB
          policies:
            - tenantId: t1002
              recordFilter: '{"type":"RELATION","name":"testCondition1","fn":{"expression":"cts > 1690345000000 && __properties__.__online__.connected == true"}}'
              fieldFilter:
          properties: { }
      - id: s1003
        name: "subscriber_3"
        enable: true
        rule:
          isSequence: false
          logRetentionTime: 1d
          logRetentionSize: 10GB
          policies:
            - tenantId: t1001
              recordFilter: '{"type":"RELATION","name":"testCondition1","fn":{"expression":"cts > 1690345000000 && __properties__.__online__.connected == true"}}'
              fieldFilter: 'del(.__properties__.__online__.connected)'
            - tenantId: t1003
              recordFilter: '{"type":"RELATION","name":"testCondition1","fn":{"expression":"cts > 1690345000000 && __properties__.__online__.connected == true"}}'
              fieldFilter:
          properties: { }

    tenants:
      - id: t1001
        name: "tenant_1"
        sourceName: kafka01
        enable: true
        properties: { }
      - id: t1002
        name: "tenant_2"
        sourceName: kafka01
        enable: true
        properties: { }
      - id: t1003
        name: "tenant_3"
        sourceName: kafka02
        enable: true
        properties: { }

  coordinator:
    shardingStrategy: AVG_SHARDING
    bootstrapServers: ${IT_KAFKA_SERVERS_01:localhost:9092}
    configConfig:
      topic: "test_streamconnect_coordinator_config_topic"
      producerProps: {}
      consumerProps: {}
    discoveryConfig:
      groupId: "test_streamconnect_coordinator_discovery_group"
      consumerProps: {}
      adminProps: {}

  pipelines:
    - name: pipeline_1
      enable: true # By default: true
      checkpoint: checkpoint_1
      source: subscribeSource_1
      # TODO support custom multiple stages processing
      #processes:
      #  - subscribeFilter_1
      #  - subscribeMapper_1
      ##  - subscribeFilter_2
      ##  - subscribeMapper_2
      filters:
        - subscribeFilter_1
      mappers:
        - subscribeMapper_1
      sink: sink_1
